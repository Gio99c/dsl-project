{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing import  load, extract_features, text_mining_tfdf, text_mining_sentiment, add_word_embeddings, clean_text, add_user_text, drop_duplicates, convert_categorical, normalize, save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of the two function that I have implemented:\n",
    " 1) compare_models --> train and test the models contained in the model list and create a dataframe to easily visualize the metrics \n",
    " 2) compare_models_KF --> do exactly the same, but it uses the K-Fold method to make robust result of the metrcs that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, models: list, names: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X_train = training data set\n",
    "        X_test = testing data set\n",
    "        y_train = training label \n",
    "        y_test = testing label \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores, precision_scores, recall_scores, accuracy_scores = [], [], [],[]\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('\\n')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(acc)\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average = 'macro')\n",
    "        precision_scores.append(prec)\n",
    "\n",
    "        rec = recall_score(y_test, y_pred, average='macro')\n",
    "        recall_scores.append(rec)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    col3 = pd.Series(recall_scores)\n",
    "    col4 = pd.Series(accuracy_scores)\n",
    "    col5 = pd.Series(precision_scores)\n",
    "\n",
    "    result = pd.concat([col1, col2, col3, col4, col5], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score', 'Recall', 'Accuray', 'Precision']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_KF(X: pd.DataFrame, y: pd.Series, models: list, names: list, k = int) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X = training data set\n",
    "        y = training label  \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "        k = int for the cross validation \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        f1_score = cross_val_score(clf, X, y, cv = k, scoring = 'f1_macro')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        f1 = f1_score.mean()\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    \n",
    "\n",
    "    result = pd.concat([col1, col2], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_models = [LinearSVC(), RandomForestClassifier(), BernoulliNB()]\n",
    "\n",
    "clf_names = ['Linear SVC', 'Random Forest', ' Bernoulli Nayve Bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load(filepath=\"./DSL2122_january_dataset/development.csv\").pipe(extract_features).pipe(drop_duplicates, drop_long_text=True).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "X_test = load(filepath=\"./DSL2122_january_dataset/evaluation.csv\").pipe(extract_features).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "\n",
    "X_train, X_test = text_mining_tfdf(X_train, X_test) #if you don't want to use it, just comment it out\n",
    "\n",
    "X_train, X_test = add_word_embeddings(X_train, X_test) #if you don't want to use it, just comment it out\n",
    "\n",
    "X_train = X_train.pipe(convert_categorical) #necessary\n",
    "X_test = X_test.pipe(convert_categorical)   #necessary\n",
    "\n",
    "X_train, X_test, y_train = normalize(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
