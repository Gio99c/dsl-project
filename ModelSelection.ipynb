{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Preprocessing import  load, cleaning, text_mining, preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of the two function that I have implemented:\n",
    " 1) compare_models --> train and test the models contained in the model list and create a dataframe to easily visualize the metrics \n",
    " 2) compare_models_KF --> do exactly the same, but it uses the K-Fold method to make robust result of the metrcs that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, models: list, names: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X_train = training data set\n",
    "        X_test = testing data set\n",
    "        y_train = training label \n",
    "        y_test = testing label \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores, precision_scores, recall_scores, accuracy_scores = [], [], [],[]\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('\\n')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(acc)\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average = 'macro')\n",
    "        precision_scores.append(prec)\n",
    "\n",
    "        rec = recall_score(y_test, y_pred, average='macro')\n",
    "        recall_scores.append(rec)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    col3 = pd.Series(recall_scores)\n",
    "    col4 = pd.Series(accuracy_scores)\n",
    "    col5 = pd.Series(precision_scores)\n",
    "\n",
    "    result = pd.concat([col1, col2, col3, col4, col5], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score', 'Recall', 'Accuray', 'Precision']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_KF(X: pd.DataFrame, y: pd.Series, models: list, names: list, k = int) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X = training data set\n",
    "        y = training label  \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "        k = int for the cross validation \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        f1_score = cross_val_score(clf, X, y, cv = k, scoring = 'f1_macro')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        f1 = f1_score.mean()\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    \n",
    "\n",
    "    result = pd.concat([col1, col2], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "clf_models = [DecisionTreeClassifier(), KNeighborsClassifier(), SVC(), RandomForestClassifier(), GaussianNB(), MLPClassifier()]\n",
    "\n",
    "clf_names = ['Decision Tree', 'K-Nearest Neighbor', 'Support Vector Machine', 'Random Forest', 'Nayve Bayes', 'Multi Layer Neural Network']\n",
    "\n",
    "y, X = (load().pipe(cleaning).pipe(text_mining).pipe(preprocessing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model: Decision Tree\n",
      "\n",
      "\n",
      "Finishing training model: Decision Tree, trained in 2.902170181274414\n",
      "\n",
      "Score of Decision Tree model performed: 1.0\n",
      "Start training model: K-Nearest Neighbor\n",
      "\n",
      "\n",
      "Finishing training model: K-Nearest Neighbor, trained in 3.1604809761047363\n",
      "\n",
      "Score of K-Nearest Neighbor model performed: 0.9999544306428063\n",
      "Start training model: Support Vector Machine\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "df_result = compare_models(X_train= X_train, X_test= X_test, y_test=y_test, y_train=y_train, models= clf_models, names= clf_names)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
