{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Preprocessing import  load, extract_features, text_mining_tfdf, text_mining_sentiment, add_word_embeddings, clean_text, add_user_text, drop_duplicates, convert_categorical, normalize, save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of the two function that I have implemented:\n",
    " 1) compare_models --> train and test the models contained in the model list and create a dataframe to easily visualize the metrics \n",
    " 2) compare_models_KF --> do exactly the same, but it uses the K-Fold method to make robust result of the metrcs that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, models: list, names: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X_train = training data set\n",
    "        X_test = testing data set\n",
    "        y_train = training label \n",
    "        y_test = testing label \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores, precision_scores, recall_scores, accuracy_scores = [], [], [],[]\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('\\n')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(acc)\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average = 'macro')\n",
    "        precision_scores.append(prec)\n",
    "\n",
    "        rec = recall_score(y_test, y_pred, average='macro')\n",
    "        recall_scores.append(rec)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    col3 = pd.Series(recall_scores)\n",
    "    col4 = pd.Series(accuracy_scores)\n",
    "    col5 = pd.Series(precision_scores)\n",
    "\n",
    "    result = pd.concat([col1, col2, col3, col4, col5], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score', 'Recall', 'Accuray', 'Precision']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_KF(X: pd.DataFrame, y: pd.Series, models: list, names: list, k = int) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X = training data set\n",
    "        y = training label  \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "        k = int for the cross validation \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        f1_score = cross_val_score(clf, X, y, cv = k, scoring = 'f1_macro')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        f1 = f1_score.mean()\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    \n",
    "\n",
    "    result = pd.concat([col1, col2], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l_/b_fbvmv10b1921g0qq09pcr80000gn/T/ipykernel_5798/425080457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Linear SVC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Bernoulli Nayve Bayes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_user_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "clf_models = [LinearSVC(), RandomForestClassifier(), BernoulliNB()]\n",
    "\n",
    "clf_names = ['Linear SVC', 'Random Forest', ' Bernoulli Nayve Bayes']\n",
    "\n",
    "X, y = load().pipe(extract_date).pipe(clean_text).pipe(add_user_text).pipe(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  251012\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1648931 lr:  0.000000 avg.loss:  0.358665 ETA:   0h 0m 0s\n",
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model: Linear SVC\n",
      "\n",
      "\n",
      "Finishing training model: Linear SVC, trained in 3.4832990169525146\n",
      "\n",
      "Score of Linear SVC model performed: 0.809720948247317\n",
      "Start training model: Random Forest\n",
      "\n",
      "\n",
      "Finishing training model: Random Forest, trained in 25.77042007446289\n",
      "\n",
      "Score of Random Forest model performed: 0.8259210688737337\n",
      "Start training model:  Bernoulli Nayve Bayes\n",
      "\n",
      "\n",
      "Finishing training model:  Bernoulli Nayve Bayes, trained in 0.07192516326904297\n",
      "\n",
      "Score of  Bernoulli Nayve Bayes model performed: 0.36679438058748404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)\n",
    "X_train, X_test = add_word_embeddings(X_train, X_test, y_train)\n",
    "\n",
    "df_result = compare_models(X_train= X_train, X_test= X_test, y_test=y_test, y_train=y_train, models= clf_models, names= clf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_negativity</th>\n",
       "      <th>embedding_positivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.566800e-01</td>\n",
       "      <td>0.843320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.715825e-01</td>\n",
       "      <td>0.128418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.189675e-01</td>\n",
       "      <td>0.881032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.133610e-01</td>\n",
       "      <td>0.386639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.428984e-01</td>\n",
       "      <td>0.457102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178479</th>\n",
       "      <td>9.211846e-02</td>\n",
       "      <td>0.907881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178480</th>\n",
       "      <td>5.627953e-09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178481</th>\n",
       "      <td>3.120898e-05</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178482</th>\n",
       "      <td>7.022826e-01</td>\n",
       "      <td>0.297717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178483</th>\n",
       "      <td>3.526465e-03</td>\n",
       "      <td>0.996473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178484 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        embedding_negativity  embedding_positivity\n",
       "0               1.566800e-01              0.843320\n",
       "1               8.715825e-01              0.128418\n",
       "2               1.189675e-01              0.881032\n",
       "3               6.133610e-01              0.386639\n",
       "4               5.428984e-01              0.457102\n",
       "...                      ...                   ...\n",
       "178479          9.211846e-02              0.907881\n",
       "178480          5.627953e-09              1.000000\n",
       "178481          3.120898e-05              0.999969\n",
       "178482          7.022826e-01              0.297717\n",
       "178483          3.526465e-03              0.996473\n",
       "\n",
       "[178484 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.073850    embedding_positivity\n",
       "0.070641                     ids\n",
       "0.067279    embedding_negativity\n",
       "0.058744                     neg\n",
       "0.048191                compound\n",
       "0.025772                polarity\n",
       "0.025334              char_count\n",
       "0.022674                     pos\n",
       "0.022161            day_of_month\n",
       "0.019548             day_of_week\n",
       "0.017202                       .\n",
       "0.016374             hour_of_day\n",
       "0.016332                       @\n",
       "0.014284                      wa\n",
       "0.013901                     neu\n",
       "0.013485            subjectivity\n",
       "0.007743           month_of_year\n",
       "0.005653                  though\n",
       "0.005349                     see\n",
       "0.005129                     ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "#f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "#print(f1)\n",
    "importances = pd.Series(X_train.columns, clf.feature_importances_)\n",
    "importances.sort_index(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 118.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1325.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3417.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 118.73it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3623.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3595.69it/s]\n",
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "Read 0M words\n",
      "Number of words:  880\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:    7252 lr:  0.000000 avg.loss:  0.694575 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "X_train = load(filepath=\"./DSL2122_january_dataset/development.csv\").sample(100).pipe(extract_features).pipe(drop_duplicates, drop_long_text=True).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "X_test = load(filepath=\"./DSL2122_january_dataset/evaluation.csv\").sample(100).pipe(extract_features).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "\n",
    "X_train, X_test = text_mining_tfdf(X_train, X_test) #if you don't want to use it, just comment it\n",
    "\n",
    "X_train, X_test = add_word_embeddings(X_train, X_test) #if you don't want to use it, just comment it\n",
    "\n",
    "X_train = X_train.pipe(convert_categorical) #necessary\n",
    "X_test = X_test.pipe(convert_categorical)   #necessary\n",
    "\n",
    "X_train, X_test, y_train = normalize(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
