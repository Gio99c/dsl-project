{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Preprocessing import  load, cleaning, text_mining, preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of the two function that I have implemented:\n",
    " 1) compare_models --> train and test the models contained in the model list and create a dataframe to easily visualize the metrics \n",
    " 2) compare_models_KF --> do exactly the same, but it uses the K-Fold method to make robust result of the metrcs that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series, models: list, names: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X_train = training data set\n",
    "        X_test = testing data set\n",
    "        y_train = training label \n",
    "        y_test = testing label \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores, precision_scores, recall_scores, accuracy_scores = [], [], [],[]\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('\\n')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(acc)\n",
    "\n",
    "        prec = precision_score(y_test, y_pred, average = 'macro')\n",
    "        precision_scores.append(prec)\n",
    "\n",
    "        rec = recall_score(y_test, y_pred, average='macro')\n",
    "        recall_scores.append(rec)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    col3 = pd.Series(recall_scores)\n",
    "    col4 = pd.Series(accuracy_scores)\n",
    "    col5 = pd.Series(precision_scores)\n",
    "\n",
    "    result = pd.concat([col1, col2, col3, col4, col5], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score', 'Recall', 'Accuray', 'Precision']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_KF(X: pd.DataFrame, y: pd.Series, models: list, names: list, k = int) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Inputs:\n",
    "        X = training data set\n",
    "        y = training label  \n",
    "        models = list of models to compare\n",
    "        names = list of trings containing the names of the models \n",
    "        k = int for the cross validation \n",
    "\n",
    "        Return: comparison table \n",
    "        \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    \n",
    "    for clf, name in zip(models, names):\n",
    "\n",
    "        print(f'Start training model: {name}')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        f1_score = cross_val_score(clf, X, y, cv = k, scoring = 'f1_macro')\n",
    "\n",
    "        finish_time = time.time()\n",
    "\n",
    "        print(f'Finishing training model: {name}, trained in {finish_time-start_time}\\n')\n",
    "\n",
    "        f1 = f1_score.mean()\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f'Score of {name} model performed: {f1}')\n",
    "\n",
    "    col1 = pd.Series(names)\n",
    "    col2 = pd.Series(f1_scores)\n",
    "    \n",
    "\n",
    "    result = pd.concat([col1, col2], axis = 'columns')\n",
    "    result.columns = ['Model Name', 'F1 Score']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_test_split() got an unexpected keyword argument 'testsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba8c4787f2f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_mining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclf_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclf_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_test_split() got an unexpected keyword argument 'testsize'"
     ]
    }
   ],
   "source": [
    "clf_models = [DecisionTreeClassifier(), KNeighborsClassifier(), SVC(), RandomForestClassifier(), GaussianNB(), MLPClassifier()]\n",
    "\n",
    "clf_names = ['Decision Tree', 'K-Nearest Neighbor', 'Support Vector Machine', 'Random Forest', 'Nayve Bayes', 'Multi Layer Neural Network']\n",
    "\n",
    "y, X = (load().pipe(cleaning).pipe(text_mining).pipe(preprocessing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model: Decision Tree\n",
      "\n",
      "\n",
      "Finishing training model: Decision Tree, trained in 40.64679288864136\n",
      "\n",
      "Score of Decision Tree model performed: 1.0\n",
      "Start training model: K-Nearest Neighbor\n",
      "\n",
      "\n",
      "Finishing training model: K-Nearest Neighbor, trained in 29.09661102294922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "df_result = compare_models(X_train= X_train, X_test= X_test, y_test=y_test, y_train=y_train, models= clf_models, names= clf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
