{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from preprocessing.utils import load_dataset, save_results\n",
    "from preprocessing.preprocessing import normalize, drop_columns, drop_duplicates, extract_date_features\n",
    "from preprocessing.sent_analysis import text_mining_sentiment\n",
    "from preprocessing.tf_df import text_mining_tfdf\n",
    "from preprocessing.word_embeddings import add_word_embeddings, print_best_params\n",
    "from preprocessing.text_cleaning import count_characters, clean_text, add_user_text\n",
    "\n",
    "\n",
    "from model_selection import test_diff_preprocessing\n",
    "\n",
    "from tuning import PARAMETERS_HGBC, PARAMETERS_RF, tuning_classifiers\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters for FastText\n",
    "\n",
    "#X_train = load_dataset().pipe(extract_date_features).pipe(drop_duplicates).pipe(count_characters, trainset = True).pipe(clean_text).pipe(add_user_text).pipe(print_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the development and evaluation set. It takes about 30 minutes to preprocess all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223343/223343 [14:18<00:00, 260.03it/s]\n",
      "100%|██████████| 223343/223343 [00:26<00:00, 8279.86it/s]\n",
      "100%|██████████| 223343/223343 [00:26<00:00, 8389.28it/s]\n",
      "100%|██████████| 74999/74999 [04:46<00:00, 262.23it/s]\n",
      "100%|██████████| 74999/74999 [00:09<00:00, 8224.82it/s]\n",
      "100%|██████████| 74999/74999 [00:09<00:00, 8298.43it/s]\n",
      "/home/christian/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/christian/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 3M words\n",
      "Number of words:  196361\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  120483 lr:  0.000000 avg.loss:  0.372608 ETA:   0h 0m 0s 0.142412 avg.loss:  0.470164 ETA:   0h 0m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Normalizing\n",
      "Finish Normalizing\n"
     ]
    }
   ],
   "source": [
    "filename = \"./DSL2122_january_dataset/evaluation.csv\"\n",
    "\n",
    "X_train = load_dataset().pipe(extract_date_features).pipe(drop_duplicates).pipe(count_characters, trainset = True).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "X_test = load_dataset(filepath=filename).pipe(extract_date_features).pipe(count_characters, trainset =False).pipe(clean_text).pipe(text_mining_sentiment).pipe(add_user_text)\n",
    "\n",
    "X_train, X_test = text_mining_tfdf(X_train, X_test, min_df=0.01) \n",
    "\n",
    "X_train, X_test = add_word_embeddings(X_train, X_test)\n",
    "\n",
    "X_train, X_test = drop_columns(X_train, X_test=X_test)\n",
    "\n",
    "\n",
    "print('Start Normalizing')\n",
    "X_train, X_test, y_train = normalize(X_train, X_test)\n",
    "print('Finish Normalizing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Finish Training\n",
      "File Salvato con questo nome: Results/HGBC_results.csv!\n"
     ]
    }
   ],
   "source": [
    "clf = HistGradientBoostingClassifier(early_stopping=False, l2_regularization=0.3, loss='binary_crossentropy',\n",
    "                                    max_iter=150, max_leaf_nodes=30, min_samples_leaf=4, random_state=42)\n",
    "\n",
    "clf_name = \"Hist Gradient Boost\"\n",
    "\n",
    "file_name = \"Results/HGBC_results.csv\"\n",
    "\n",
    "print('Start Training')\n",
    "clf.fit(X_train,y_train)\n",
    "print('Finish Training')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "save_results(y_pred, fp = file_name)\n",
    "print(f\"File Salvato con questo nome: {file_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Finish Training\n",
      "File Salvato con questo nome: Results/RF_results.csv!\n"
     ]
    }
   ],
   "source": [
    "clf =  RandomForestClassifier(max_features='log2', min_samples_leaf=10, min_samples_split=9, n_estimators=500, random_state=42)\n",
    "\n",
    "clf_name = \"Random Forest Classifier\"\n",
    "\n",
    "file_name = \"Results/RF_results.csv\"\n",
    "\n",
    "print('Start Training')\n",
    "clf.fit(X_train,y_train)\n",
    "print('Finish Training')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "save_results(y_pred, fp = file_name)\n",
    "print(f\"File Salvato con questo nome: {file_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.339118    embedding_negativity\n",
       "0.332505    embedding_positivity\n",
       "0.086819                     ids\n",
       "0.039278                compound\n",
       "0.033823                     neg\n",
       "0.023624                polarity\n",
       "0.020008                     pos\n",
       "0.019655            day_of_month\n",
       "0.014549                     not\n",
       "0.012946           month_of_year\n",
       "0.007014                     neu\n",
       "0.004260             hour_of_day\n",
       "0.004185              char_count\n",
       "0.003675            subjectivity\n",
       "0.003643             day_of_week\n",
       "0.002973                     sad\n",
       "0.002726                    wish\n",
       "0.002708                     but\n",
       "0.002605                  thanks\n",
       "0.002090                    want\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.Series(X_train.columns, clf.feature_importances_)\n",
    "importances.sort_index(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf= RandomForestClassifier(n_estimators= 500, random_state=42)\n",
    "tuning_classifiers(clf, PARAMETERS_RF, X_train, y_train, k_fold = 3, normal_grid_search = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(random_state=42,loss= \"binary_crossentropy\")\n",
    "tuning_classifiers(clf, PARAMETERS_HGBC, X_train, y_train, k_fold=3, normal_grid_search= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= RandomForestClassifier(n_estimators= 500, random_state=42)\n",
    "tuning_classifiers(clf, PARAMETERS_RF, X_train, y_train, k_fold = 3, normal_grid_search = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc3, preproc2, preproc1 = test_diff_preprocessing(X_train= X_train, y_train= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" 3 TECHNIQUES (WORD EMBEDDINGS, TF-DF, SENTYMENT): {preproc3}\")\n",
    "print(f\" 2 TECHNIQUES (WORD EMBEDDINGS, TF-DF, SENTYMENT): {preproc2}\")\n",
    "print(f\" 1 TECHNIQUE (SENTYMENT): {preproc1}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
