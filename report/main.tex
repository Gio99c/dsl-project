\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{tablefootnote}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{caption}
\usepackage{svg}
\graphicspath{ {./figures/} }
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage[autostyle]{csquotes}
\begin{document}

\title{\textit{Tweets} Sentiment Analysis}

\author{\IEEEauthorblockN{Giuseppe Concialdi}
\IEEEauthorblockA{\textit{Politecnico di Torino} \\
Student id: s294666 \\
giuseppe.concialdi@studenti.polito.it}
\and
\IEEEauthorblockN{Christian Montecchiani}
\IEEEauthorblockA{\textit{Politecnico di Torino} \\
Student id: s303681 \\
christian.montecchiani@studenti.polito.it}
}

\maketitle

\begin{abstract}
In this report, we developed a model that is able to classify the sentiments of Twitter's tweets.
We focused on text cleaning and information retrieval within the tweets employing advanced feature extraction techniques.
We exploited high-performance algorithms that could handle such a big dataset and could accomplish the classification in a reasonable amount of time.
\end{abstract}

\section{Problem overview}\label{sec:overview}
The objective of the competition is to build a model that is able to classify whether a tweet contains positive or negative sentiments. The dataset provided is arranged as follow:
\begin{itemize}
    \item A \textbf{development} set composed by 224,994 records of tweets. Each sample has six different features, including the \textit{sentiment} attribute that is the target of the classification.
    \item A \textbf{evaluation} set consisting of 74,999 samples. Its dimension is one-third of the development set and it does not feature the target variable.
\end{itemize}
Every sample is characterized by:
\begin{itemize}
    \item \textit{ids}: the unique identifier of the tweet. It is represented by a progressive integer number that is related to the timestamp of the tweet. By digging in the past Twitter documentation, we found out that the tweet id is generated in a snowflake format\cite{snowflake}, invented by Twitter for the generation of sequential ids for their tweets. The timestamp is encoded within the id as shown in Figure \ref{fig:snowflake}.
    \begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{snowflake}
        \caption{Structure of a 64-bit snowflake id.}
        \label{fig:snowflake}
\end{figure}
    \item \textit{date}: the timestamp of each tweet. The date is encoded as a string with the format:
    $$weekday \quad month \quad day \quad hour:min:sec \quad tz \quad year$$
    The dates in the dataset range from April 6 to June 25 of 2009.
    \item \textit{flag}: a string whose significance is unsure. It is featured in the whole dataset with the unique value of \texttt{NO\_QUERY}. Given its absence of meaning, this feature will be removed without a second thought, but it is possible that it would report the query used to retrieve the tweets when they were extracted using some Twitter APIs.
    \item \textit{user}: the username of the creator of the tweet. Even though there are almost 225 thousand tweets, there are only 10,647 different usernames.
    \item \textit{text}: the text of the tweet. This is the core part of the analysis, it embodies a lot of insights that can be extracted and analyzed to retrieve the overall polarity of the tweet's sentiments. In 2009 the maximum length of a tweet was 140 characters\cite{tweet_lenght}. However Figure \ref{fig:charcount}, shows that some tweets exceed this threshold, so it is likely to have issues with the encoding of the text extracted.
    \item \textit{sentiment}: the target variable of the classification. It assumes two possible integer values: 0 and 1 that represent respectively negative and positive sentiments. The dataset is fairly unbalanced, as shown in Figure \ref{fig:unbalanced}: there are more positive sentiments than negative ones.
\end{itemize}
\begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{charcount}
        \caption{Boxplot showing the number of chacters of the tweets per sentiment}
        \label{fig:charcount}
\end{figure}
\begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{unbalanced}
        \caption{Distribution of the tweets' sentiments in the training dataset}
        \label{fig:unbalanced}
\end{figure}
The dataset does not feature any missing value, but it contains redundant information that is useless for the analysis.

\section{Proposed approach}\label{sec:approach}
The preprocessing was characterized by the features extraction and the text mining phases.

We decided to extract the time-related information from the \textit{date} attribute and figure out later if their relative importance would matter.
%At this stage was still unknown whether all these attributes extracted from the \textit{date} variable would have been useful or not. That is because the \textit{ids} feature is not useless, but it already encodes the timestamp of the tweet.

Although the \textit{date} attribute could hide some useful insights about the sentiment of the tweet, the major knowledge lies within the \textit{text} attribute. The information extraction from this feature can be performed in different ways and with different processes. We decided to tackle the problem from different points of view and we managed to embed all this data into the model. Firstly, we cleaned up the tweets' text, then we exploited the text performing:
\begin{itemize}
    \item A \textbf{Sentiment Intensity Analysis}\cite{sentiment_analysis} on the text, obtaining new features on the polarization of the sentiments.
    \item A \textbf{TF-DF} %\footnote{Term Frequency - Document Frequency. The tf-df of term $t$ in document $d$ of collection $D$ is computed as: $$tfdf(t) = freq(t,d) * \log(freq(t,D)$$}
    of the text in order to get the words that mainly influence the sentiments of the tweets.
    \item A \textbf{Word Embeddings}\cite{wordembeddings} approach with the FastText\cite{fasttext} library to retrieve the morphological relationships between the words in a sentence.
\end{itemize}
\begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{overall_schema}
        \caption{Overall schema of the problem approach}
        \label{fig:overall_schema}
\end{figure}
Figure \ref{fig:overall_schema} shows a summary of our approach.
The last technique resulted in a very powerful tool, that is able, on its own, to perform the classification of the sentiment of the evaluation set with a macro F1-score\footnote{The F1-score is the harmonic mean of the precision and recall therefore it is defined for a single class. With macro F1-score we refer to the mean of all classes' F1-scores.}
higher than 0.8. 

\subsection{Preprocessing}\label{sec:preprocessing}
The preprocessing step is the core phase of the entire analysis. Here we performed the extraction, transformation and normalization of the features. As discussed in Section \ref{sec:approach}, we extracted from the \textit{date} attribute all the relevant information and we discarded the properties that are redundant like \textit{year}, \textit{timezone} and \textit{flag}. Then we decided to add a new feature based on the number of characters of the tweet. The new \textit{char\_count} attribute is useful to troubleshoot the tweets' texts that are longer than 140 characters and it is also valuable for the classification itself.

Afterwards, we addressed the tweet duplication issue. There are three kinds of duplicated tweets:
\begin{enumerate}
    \item Same \textit{text}, \textbf{different} \textit{author}, \textbf{same} \textit{sentiment}
    \item Same \textit{text}, \textbf{same} \textit{author}, \textbf{same} \textit{sentiment}
    \item Same \textit{text}, \textbf{different} \textit{sentiment}
\end{enumerate}
For the first kind of duplicated tweets, no action is required. Those are very common and eventually short sentences that different authors twitted. They share the same sentiment so it enforces the model to learn that those phrases belong to one specific class. The second kind are tweets that have been posted more than once by the same author. This could be a mistake or maybe the user wanted to retweet the same post. Either way, we decided to keep only one copy of the duplicated post. The most controversial kind of duplication is the last one: regardless of the author, if the same text is labelled differently, means that the data is mistaken.
For this reason, we decided to drop all the records with these conflicting properties.

\begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{cleaning_schema}
        \caption{Text cleaning pipeline}
        \label{fig:cleaning_schema}
\end{figure}
Then, we tackled the cleaning of the tweets' text. To properly perform this step, we took into consideration a lot of alternatives. Different elements characterize a Twitter post and we tried to draw every possible bit of information from the text. The tweets' length issue, led us to inspect the text in search of an encoding-related problem. We found out that the text extracted seems to be encoded in UTF-8\cite{utf8} but some special symbols were parsed as HTML entities\footnote{Character entities are used to display reserved characters in HTML}. We employed the HTML library to clean up the text. After that, we lowered the case of the words and extracted the domain name from the URLs contained in the tweets. We exploited three vocabularies\footnote{{We reworked existing libraries to be more suitable for the preprocessing. The \texttt{slangs}, \texttt{contractions} and \texttt{stopwords} dictionaries derive respectively from \textbf{Deffro}'s repository\cite{deffro}, \textbf{pycontractions} and \textbf{NLKT} libraries.}} to expand into clear words the emoticons and the slang terms. By substituting these pieces of text in addition to some regex to regularize the phrases we obtained a much more coherent and convenient format for the analysis. Ultimately, we applied the lemmatization\cite{lemmatization} to extract the lemma from each word. We preferred lemmatizing to stemming\cite{stemming} to preserve the morphology of the words\cite{stemmingVSlemmatizing}.

Figure \ref{fig:cleaning_schema} shows the cleaning pipeline for the \textit{text} attribute.

At this stage, the \textit{text} feature is ready to be processed. We heavily relied on the tools offered by the NLKT library\cite{nlkt} to carry on the analysis. We employed the Vader Sentiment Intensity Analyzer\cite{vader} to gain the relative frequencies of \textit{positive}, \textit{neutral} and \textit{negative} terms. Besides, also the \textit{compound} value is stored, which is a single value that summarises the three frequencies. Following, we used the TextBlob library\cite{textblob} to extract additional features: the \textit{subjectivity} score and the \textit{polarity} index. These variables are correlated among them, but they describe the same phenomenon under different points of view and complement each other.

Going forward, we applied a TF-DF to extract words with a relatively high minimum support. Our aim was to retrieve the words that were present in a lot of tweets and which meaning was heavily polarized toward one class. In this case, we did not use a normal stop words vocabulary, because there are a lot of common terms that are very important for the classification, like negations. So we created a custom vocabulary that contained only conjunctions and some adverbs. The most frequent word resulting from the TF-DF are shown in Figure \ref{fig:word_cloud}.
\begin{figure}[h]
        \centering
        \includegraphics[width=\columnwidth]{word_cloud}
        \caption{Word Cloud generated by the results of the TF-DF. The figure has been created using the wordcloud library\cite{word_cloud}}
        \label{fig:word_cloud}
\end{figure}

Finally, we leveraged the word embeddings approach through Meta's FastText library. %The vocabulary was built on top of the tweets of the development set. The autotuning feature of the library allowed us to treat the neural network underneath the facade as a black box. We split the development data into training and validation and the model tuned itself by maximizing the resulting F1-score.
One of the strengths of this technique is that it does not suffer the presence of unfamiliar terms because it treats the words as multi-dimensional vectors. The outputs of the neural network are the predictions and the softmax likelihoods for each class. We extracted these probabilities and we integrated them into the dataset.


We tried to operate with the \textit{user} attribute to retrieve some useful information within it. We thought that due to the relative low cardinality of this attribute (only 10,000 different usernames) the tweets posted by the same author may reflect its personality, thus it is likely that a sentiment is predominant to the other one. Nonetheless, the One-Hot-Encoding\cite{ohe} of these features would have resulted in a huge dimensionality increase of the dataset. So, we chose to incorporate the user author of the post at the beginning of the text of the tweet. The text already contains mentioned user whose name is preceded by an at-sign (@). In this way, the word embedding classification will retrieve information about the author of the post like if he was mentioned within it.
\begin{figure}[h]
        \centering
        \includesvg[width=\columnwidth]{text_schema}
        \caption{Feature extraction schema of \textit{text} attribute}
        \label{fig:text_schema}
\end{figure}

Figure \ref{fig:text_schema} summarises the operations performed on the \textit{text} attribute.

The data is not yet ready to be processed by a classification algorithm. The categorical attributes need to be addressed. We removed the \textit{text} and the \textit{user} features because we have already extracted the information needed and then we proceeded to map the remaining features into numerical ones. All the columns are normalized with a MinMax scaler\cite{minmax}.
\subsection{Model selection}\label{sec:model}
We chose four different algorithms to perform the classification task and we compared their performances to assess which are the best classifiers:
\begin{itemize}
    \item \textbf{Random Forest Classifier\cite{rf}}: ensemble of decision trees.
    \item \textbf{Bernoulli Naïve Bayes\cite{bnb}\footnote{We tested the Naïve Bayes Classifier and, as we expected, it was the worst classifier. This shows that the features are indeed correlated}}: classifier for multivariate Bernoulli\cite{bernoulli} models.
    \item \textbf{Linear Support Vector Classifier\cite{svc}}: linear kernel version of the more general SVC.
    \item \textbf{Histogram-based Gradient Boosting Classification Tree\cite{histgtb}}: high-performance implementation for big datasets of the Gradient Tree Boosting Classifier\cite{gtb}. 
\end{itemize}
To test the effectiveness of our preprocessing, we tried to test the performances of the models by increasingly combining the results of the different techniques applied to the \textit{text} attribute. 
\begin{table}[h]
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Techniques}                                     & \textbf{Classifier}                       & \textbf{F1-score} \\ \midrule
\multirow{4}{*}{\textit{Sentiment}}                     & Linear SVC                       & 0,723    \\
                                               & Random Forest         & 0,737    \\
                                               & Bernoulli Naive-Bayes            & 0,663    \\
                                               & Hist Gradient Boosting & 0,749    \\ \midrule
\multirow{4}{*}{\textit{Sentiment + TF-DF}}             & Linear SVC                       & 0,762    \\
                                               & Random Forest         & 0,780    \\
                                               & Bernoulli Naive-Bayes            & 0,713    \\
                                               & Hist Gradient Boosting & 0,786    \\ \midrule
\multirow{4}{*}{\textit{Sentiment + TF-DF + Word Emb.}} & Linear SVC                       & 0,896    \\
                                               & Random Forest         & 0,903    \\
                                               & Bernoulli Naive-Bayes            & 0,713    \\
                                               & Hist Gradient Boosting & 0,904    \\ \bottomrule
\end{tabular}
\caption{Results of the model selection phase.}
\label{tab:techniques}
\end{table}
As shown in Table \ref{tab:techniques}, at first we applied only the sentiment extraction technique, then we joined it with the TF-DF outcomes and in the last trial we added the word embeddings' likelihoods into the equation. It is possible to see an increase in the performance by matching the features. The highest score is reached by bringing together all the extracted features with the different methods.

During this phase, we exploited the Random Forest feature importances to understand what are the most relevant features upon which the algorithm is splitting the samples. Accordingly to our results, we found out that the most significant attributes are the likelihood resulting from the word embeddings classification, following there are the sentiment features and lastly some very polarized words coming from the TF-DF. We also discovered that the \textit{ids} attribute was always more relevant than the features extracted from the date. However, we decided to keep those features because their relevance was still high.

\subsection{Hyperparameters tuning}\label{sec:hypertuning}
From the model selection phase, the best dataset turned out to be the \textit{word embedding} + \textit{sentiment extraction} + \textit{TF-DF}, therefore the models have been working with it.

We decided to tune just the two best performing models from Table \ref{tab:techniques}: \textbf{Random Forest Classifier} and \textbf{Histogram-based Gradient Boosting Classification Tree}.
\begin{table}[h]
\begin{tabular}{ccc}
\hline
\textbf{Classifier}                     & \textbf{Parameters}          & \textbf{Values}            \\ \hline
\multirow{4}{*}{Random Forest}          & \textit{max\_features}       & \{log2, sqrt\}             \\
                                        & \textit{criterion}           & \{gini, entropy\}          \\
                                        & \textit{min\_samples\_leaf}  & \{10, 20, 30, 40, 50, 60\} \\
                                        & \textit{min\_samples\_split} & \{2, 3, 4, 5, 6, 7, 8, 9\} \\ \hline
\multirow{5}{*}{Hist Gradient Boosting} & \textit{max\_iter}           & \{150, 200\}               \\
                                        & \textit{max\_leaf\_nodes}    & \{None, 20, 30, 40\}       \\
                                        & \textit{min\_samples\_leaf}  & \{2, 4, 10\}               \\
                                        & \textit{early\_stopping}     & \{True, False\}            \\
                                        & \textit{l2\_regularization}  & \{0, 0.1, 0.2, 0.3\}       \\ \hline
\end{tabular}
\caption{Hyperparameters configuration considered}
\label{tab:hypertuning}
\end{table}
The hyperparameters that we have taken into account for the tuning phase are reported in Table \ref{tab:hypertuning}.

To tune our models we used a hyperparameter optimization technique faster than the Grid Search, called Halving Grid Search\cite{halving}. It tries all the candidates with a small amount of records of the training set and iteratively selects the best candidates, using more and more records\cite{halving_explanation}. To highlight the speed of this technique we employed the same parameters grid with three folds. The results are exposed in Table \ref{tab:tuning_tech}
\begin{table}[h]
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Tuning technique}    & \textbf{Time}      & \textbf{Best private score} & \textbf{Best public score} \\ \midrule
Halving Grid Search & 12m 24s   & 0,9036             & 0,844             \\
Grid Search         & 4h 23m 5s & 0,9039             & 0,845             \\ \bottomrule
\end{tabular}
\caption{Comparison of the performances of the tuning techniques}
\label{tab:tuning_tech}
\end{table}



\section{Results}\label{sec:results}
In this section are reported the main outcomes of the hyperparameters tuning phase.
\begin{table}[h]
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Classifier}                          & \textbf{Parameters} & \textbf{Values} & \textbf{F1-Score}      \\ \midrule
\multirow{4}{*}{Random Forest}               & n\_estimators       & 500             & \multirow{4}{*}{0,845} \\
                                             & max\_features       & log2            &                        \\
                                             & min\_samples\_leaf  & 10              &                        \\
                                             & min\_samples\_split & 9               &                        \\ \midrule
\multirow{6}{*}{Hist Gradient Boosting} & max\_iter           & 150             & \multirow{6}{*}{0,846} \\
                                             & max\_leaf\_nodes    & 30              &                        \\
                                             & min\_samples\_leaf  & 4               &                        \\
                                             & loss                & binary\_crossentropy &                        \\
                                             & early\_stopping     & False           &                        \\
                                             & l2\_regularization  & 0.3             &                        \\ \bottomrule
\end{tabular}
\caption{Results of the models with the best hyperparamters configuration}
\label{tab:results}
\end{table}
Table \ref{tab:results} showcases the best configuration and the public F1-scores results for each model.
We decided to submit the evaluations performed by the Hist Gradient Boosting Tree Classifier.
\section{Discussion}\label{sec:discussion}
This classification task extensively relied upon Natural Language Processing libraries. Each library offers a wide range of tools that allows to effectively retrieve information from the text. We decided to cover more NLP libraries to benefit from the variety of materials at our disposal. Almost every library provides advanced features, such as Part Of Speech Tagging, that could have been beneficial to the classification but we decided not to include them in this analysis. 

All the classification algorithms, and the majority of utilities employed in Section \ref{sec:model} and \ref{sec:hypertuning} come from the scikit-learn packages\cite{scikit-learn}. Alongside the classical classification algorithms, we decided to try the Histogram-based Gradient Tree Boosting Classifier because, like the Random Forest Classifier, it is an ensemble of decision trees. We wanted to explore how the boosting technique would have performed for this job, and it led to very satisfactory results.
\bibliography{bibliography}
\bibliographystyle{ieeetr}\footnote{All the materials employed for this paper are available at the project repository: \url{https://github.com/Gio99c/dsl-project}}

\end{document}